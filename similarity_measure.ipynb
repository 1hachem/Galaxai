{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a proof of concept on how we intend of \n",
    "# measuring similarity between people in the community \n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "user1 = \"\"\"my name is Hachem Betrouni, I am a Data Science and AI student from Algeria,\n",
    "            currently working at InstaDeep on predicting and generating new variants of Sars-CoV-2 using GANs\"\"\"\n",
    "\n",
    "user2 = \"\"\"mohcen is an natural language processing expert from algeria, worked with instadeep \n",
    "            on predicting biological properties of protein sequences\"\"\"\n",
    "\n",
    "user3 = \"\"\"talks about innovation and startups, passionate about the food industry\"\"\"\n",
    "\n",
    "users = [user1, user2, user3]\n",
    "users = [\"[CLS] \" + user + \" [SEP]\" for user in users]\n",
    "\n",
    "tokenized_users = [tokenizer.tokenize(user) for user in users]\n",
    "\n",
    "\n",
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(tokenized_user) for tokenized_user in tokenized_users]\n",
    "segments_ids = [[1] * len(tokenized_user) for i, tokenized_user in zip(range(1,len(users)+1),tokenized_users)]\n",
    "\n",
    "tokens_tensor = [torch.tensor([indexed_token]) for indexed_token in indexed_tokens]\n",
    "segments_tensors = [torch.tensor([segments_id]) for segments_id in segments_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    hidden_states = []\n",
    "    for tokens, segments in zip(tokens_tensor, segments_tensors):\n",
    "        \n",
    "        outputs = model(tokens, segments)\n",
    "        hidden_states.append(outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = [torch.stack(h, dim=0) for h in hidden_states]\n",
    "token_embeddings = [torch.squeeze(t_e, dim=1) for t_e in token_embeddings]\n",
    "token_embeddings = [t_e.permute(1,0,2) for t_e in token_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = [h[-2][0] for h in hidden_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = [torch.mean(token_vec, dim=0) for token_vec in token_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between user1 and user2 : 0.9124594330787659\n",
      "similarity between user1 and user3 : 0.7569449543952942\n"
     ]
    }
   ],
   "source": [
    "#measuring cosine similarity \n",
    "\n",
    "#user 1 and user 2\n",
    "print(\"similarity between user1 and user2 :\", 1 - cosine(sentence_embedding[0], sentence_embedding[1]))\n",
    "#user 2 and user 3\n",
    "print(\"similarity between user1 and user3 :\", 1 - cosine(sentence_embedding[0], sentence_embedding[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('mlgraph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb829b44ae7b76ad4487b46975028f2879effc33a72d270b5819fbdff3468585"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
